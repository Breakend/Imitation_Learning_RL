1. Deep RL from Human Preferences(https://arxiv.org/pdf/1706.03741.pdf):
Pairs of trajectory segments - humans to define goals/rewards

Usually in Inverse RL - we extract the reward functions if we have demonstrations of the required task.
Imitation learning - humans to provide demonstrations - however, humans cannot always provide demonstrations for everything.

In this work - human to provide feedback (reward) based on the system's behaviour.
Humans are provided with pairs of segments of trajectories - and they evaluate which trajectory is better?
- but using human feedback directly for the reward function can be expensive.


Approach : Learn a reward function from human feedback - and then optimize the reward function.
Algorithm fits a reward function to the human preferences - while simultaneously training a policy to 
optimize the current predicted reward function.


Experiments : In Atari and MuJoCo domains.

In traditional RL - the environment provides the reward signal. In this work,
the human observer express preferences between trajectory segments. 
Trajectory segment : sequence of observations and actions. 

Goal of the agent is to provide trajectories - which will be preferred by the human, while making as few queries
as possible to the human.

We say that preferences are generated by a reward function. 

Other work : Trajectory preference queries (Wilson et al., 2012)


Selecting Queries:
Decide how to query the human preferneces (based on all these trajectory segments provided to the human)
based on approximation to the uncertainty in the reward function.
Select preferences for which the predictions have the highest variance across the ensemble members.


Experimental Details:

Also uses a synthetic oracle instead of always using a human preference.
That is, when the agent queries for a comparison, instead
of sending the query to a human, we immediately reply by indicating a preference for whichever
trajectory segment actually receives a higher reward in the underlying task


